<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models">
  <meta name="description" content="Cultural bias audit of text-to-image and image-to-image models across six countries, pairing human judgments with a culture-aware metric and releasing reproducible assets for research.">
  <meta name="keywords" content="cultural bias evaluation, generative image models, text-to-image, image-to-image editing, fairness benchmark, responsible AI">
  <meta name="author" content="Huichan Seo, Sieun Choi, Minki Hong, Yi Zhou, Junseo Kim, Lukman Ismaila, Naome Etori, Mehul Agarwal, Zhixuan Liu, Jihie Kim, Jean Oh">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:locale" content="en_US">
  <meta property="og:site_name" content="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models">
  <meta property="og:title" content="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models">
  <meta property="og:description" content="Cultural bias audit of text-to-image and image-to-image models across six countries, pairing human judgments with a culture-aware metric and releasing reproducible assets for research.">
  <meta property="og:url" content="https://cmubig.github.io/ECB/">
  <meta property="og:image" content="https://cmubig.github.io/ECB/static/images/introduction/intro.png">
  <meta property="og:image:secure_url" content="https://cmubig.github.io/ECB/static/images/introduction/intro.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="Cultural bias examples across six countries.">
  <meta property="article:published_time" content="2025-10-20T00:00:00Z">
  <meta property="article:modified_time" content="2025-10-20T00:00:00Z">
  <meta property="article:author" content="Huichan Seo">
  <meta property="article:section" content="Generative AI">
  <meta property="article:tag" content="Generative AI">
  <meta property="article:tag" content="Cultural Bias">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:site" content="@cmubiglab">
  <meta name="twitter:creator" content="@cmubiglab">
  <meta name="twitter:title" content="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models">
  <meta name="twitter:description" content="Cultural bias audit of text-to-image and image-to-image models across six countries, pairing human judgments with a culture-aware metric and releasing reproducible assets for research.">
  <meta name="twitter:image" content="https://cmubig.github.io/ECB/static/images/introduction/intro.png">
  <meta name="twitter:image:alt" content="Cultural bias examples across six countries.">
  <meta name="twitter:url" content="https://cmubig.github.io/ECB/">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models">
  <meta name="citation_author" content="Seo, Huichan">
  <meta name="citation_author" content="Choi, Sieun">
  <meta name="citation_author" content="Hong, Minki">
  <meta name="citation_author" content="Zhou, Yi">
  <meta name="citation_author" content="Kim, Junseo">
  <meta name="citation_author" content="Ismaila, Lukman">
  <meta name="citation_author" content="Etori, Naome">
  <meta name="citation_author" content="Agarwal, Mehul">
  <meta name="citation_author" content="Liu, Zhixuan">
  <meta name="citation_author" content="Kim, Jihie">
  <meta name="citation_author" content="Oh, Jean">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="Preprint">
  <meta name="citation_pdf_url" content="https://cmubig.github.io/ECB/static/pdfs/Exposing%20Blindspot.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <title>Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models | CMU BigLab</title>
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="canonical" href="https://cmubig.github.io/ECB/">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models",
    "description": "Unified audit of text-to-image and image-to-image models across six countries exposes cultural blindspots and releases reproducible evaluation assets.",
    "author": [
      {
        "@type": "Person",
        "name": "Huichan Seo",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Sieun Choi",
        "affiliation": {
          "@type": "Organization",
          "name": "Dongguk University"
        }
      },
      {
        "@type": "Person",
        "name": "Minki Hong",
        "affiliation": {
          "@type": "Organization",
          "name": "Dongguk University"
        }
      },
      {
        "@type": "Person",
        "name": "Yi Zhou",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Junseo Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "Delft University of Technology"
        }
      },
      {
        "@type": "Person",
        "name": "Lukman Ismaila",
        "affiliation": {
          "@type": "Organization",
          "name": "Johns Hopkins University School of Medicine"
        }
      },
      {
        "@type": "Person",
        "name": "Naome Etori",
        "affiliation": {
          "@type": "Organization",
          "name": "University of Minnesota Twin Cities"
        }
      },
      {
        "@type": "Person",
        "name": "Mehul Agarwal",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Zhixuan Liu",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      },
      {
        "@type": "Person",
        "name": "Jihie Kim",
        "affiliation": {
          "@type": "Organization",
          "name": "Dongguk University"
        }
      },
      {
        "@type": "Person",
        "name": "Jean Oh",
        "affiliation": {
          "@type": "Organization",
          "name": "Carnegie Mellon University"
        }
      }
    ],
    "datePublished": "2025-10-20",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://cmubig.github.io/ECB/",
    "image": "https://cmubig.github.io/ECB/static/images/introduction/intro.png",
    "keywords": ["generative AI", "cultural bias", "text-to-image", "image-to-image editing", "fairness", "evaluation"],
    "abstract": "We audit text-to-image and image-to-image models across six countries, eight categories, and era-aware prompts, combining automatic scores, a retrieval-augmented culture-aware metric, and native expert judgments to expose recurring cultural blindspots. The benchmark and full image corpus are released with prompts and settings to support reproducible cultural audits of generative systems.",
    "citation": "@article{seo2025exposing,\n  title={Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models},\n  author={Seo, Huichan and Choi, Sieun and Hong, Minki and Zhou, Yi and Kim, Junseo and Ismaila, Lukman and Etori, Naome and Agarwal, Mehul and Liu, Zhixuan and Kim, Jihie and Oh, Jean},\n  year={2025},\n  url={https://arxiv.org/submit/6893026/view}\n}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://cmubig.github.io/ECB/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Generative image models"
      },
      {
        "@type": "Thing", 
        "name": "Responsible AI"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Exposing Blindspots Project",
    "url": "https://github.com/cmubig/ECB",
    "sameAs": [
      "https://github.com/cmubig",
      "https://github.com/cmubig/ECB"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <!-- Project Resources Dropdown -->
  <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View key resources for this project">
      <i class="fas fa-flask"></i>
      Project Resources
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>Explore the Project</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        <a href="static/pdfs/Exposing%20Blindspot.pdf" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Download the Paper</h5>
            <p>Full preprint with figures, methodology, and appendices.</p>
            <span class="work-venue">PDF</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/submit/6893026/view" class="work-item" target="_blank">
          <div class="work-info">
            <h5>arXiv Submission</h5>
            <p>Live submission page with the latest revision and citation details.</p>
            <span class="work-venue">arXiv</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://github.com/cmubig/ECB" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Code & Evaluation Assets</h5>
            <p>Reproducible pipelines, prompts, and metric implementations.</p>
            <span class="work-venue">GitHub</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://www.linkedin.com/in/huichan-seo-443173265/" target="_blank" rel="noopener">Huichan Seo</a><sup>*</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://linkedin.com/in/sieunchoi/" target="_blank" rel="noopener">Sieun Choi</a><sup>*</sup>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://bk123477.github.io/" target="_blank" rel="noopener">Minki Hong</a><sup>*</sup>,&nbsp;
              </span>
              <span class="author-block"              
              <a href="https://www.linkedin.com/in/yi-z-b9698ba0/" target="_blank" rel="noopener"><a herf>Yi Zhou</a><sup>*</sup>,&nbsp;
              
              <span class="author-block">
                <a href="https://kimjunseo.com/" target="_blank" rel="noopener">Junseo Kim</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://ismailukman.github.io/" target="_blank" rel="noopener">Lukman Ismaila</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/naome22/" target="_blank" rel="noopener">Naome Etori</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://mehul.al/" target="_blank" rel="noopener">Mehul Agarwal</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://ariannaliu.github.io/" target="_blank" rel="noopener">Zhixuan Liu</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/view/jihiekim/" target="_blank" rel="noopener">Jihie Kim</a>,&nbsp;
              </span>
              <span class="author-block">
                <a href="https://www.cs.cmu.edu/~jeanoh/" target="_blank" rel="noopener">Jean Oh</a>
              </span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block">Carnegie Mellon University &middot; Dongguk University &middot; Delft University of Technology &middot; Johns Hopkins University School of Medicine &middot; University of Minnesota Twin Cities</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution</small></span>
            </div>
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/submit/6893026/view" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/cmubig/ECB" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code &amp; Data</span>
                </a>
              </span>
              <span class="link-block has-tooltip has-tooltip-arrow has-tooltip-bottom" data-tooltip="Coming soon">
                <button class="external-link button is-normal is-rounded is-dark is-disabled" disabled>
                  <span class="icon" aria-hidden="true">🤗</span>
                  <span>Hugging Face</span>
                </button>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Overview figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <figure class="image">
        <img src="static/images/introduction/intro.png" alt="Representative cultural blindspots in text-to-image generations across six countries." loading="lazy">
      </figure>
      <h2 class="subtitle has-text-centered mt-4">
        Country-agnostic prompts quickly default to Global-North aesthetics: mis-styled Indian weddings, flattened East Asian ceremony cues, wildlife stereotypes, and misplaced rituals motivate our cultural audit.
      </h2>
    </div>
  </div>
</section>
<!-- End overview figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generative image models produce striking visuals yet often misrepresent culture. Prior work has probed cultural dimensions primarily in text-to-image (T2I) systems, leaving image-to-image (I2I) editors largely underexamined. We close this gap with a unified, reproducible evaluation spanning six countries, an 8-category/36-subcategory schema, and era-aware prompts, auditing both T2I generation and I2I editing under a standardized, reproducible protocol that yields comparable model-level diagnostics.
          </p>
          <p>
            Using open models with fixed configurations, we derive comparable diagnostics across countries, eras, and categories for both T2I and I2I. Our evaluation combines standard automatic measures, a culture-aware metric that integrates retrieval-augmented VQA with curated knowledge, and expert human judgments collected on a web platform from country-native reviewers. To enable downstream analyses without re-running compute-intensive pipelines, we release the complete image corpus from both studies alongside prompts and settings.
          </p>
          <p>
            Our study reveals three recurring findings. First, under country-agnostic prompts, models default to Global-North, modern-leaning depictions and flatten cross-country distinctions, reducing separability between culturally distinct neighbors despite fixed schema and era controls. Second, iterative I2I editing erodes cultural fidelity even when conventional metrics remain flat or improve; by contrast, expert ratings and our culture-aware metric both register this degradation. Third, I2I models tend to apply superficial cues (palette shifts, generic props) rather than context- and era-consistent changes, frequently retaining source identity for Global-South targets and drifting toward non-photorealistic styles; attribute-addition trials further expose weak text rendering and brittle handling of fine, culture-specific details. Taken together, these results indicate that culture-sensitive edits remain unreliable in current systems. By standardizing prompts, settings, metrics, and human evaluation protocols—and releasing all images and configurations—we offer a reproducible, culture-centered pipeline for diagnosing and tracking progress in generative image research. Project page: <a href="https://seochan99.github.io/ECB/" target="_blank">https://seochan99.github.io/ECB/</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->





<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-portrait">
          <img src="static/images/framework.png" alt="Evaluation framework across countries, categories, and prompt eras." loading="lazy">
          <h2 class="subtitle has-text-centered">
            Schema spanning six countries, eight categories, and three era-aware prompts feeds unified T2I and I2I protocols.
          </h2>
        </div>
        <div class="item item-portrait">
          <img src="static/images/result/multi-loop-edit.png" alt="Comparison of CLIPScore and human judgment across iterative edits." loading="lazy">
          <h2 class="subtitle has-text-centered">
            Iterative edits keep CLIPScore flat while human ratings expose rapid cultural degradation.
          </h2>
        </div>
        <div class="item item-landscape">
          <img src="static/images/attribute.png" alt="Stepwise attribute addition for Korea and the United States." loading="lazy">
          <h2 class="subtitle has-text-centered">
            Attribute addition shows text rendering failures and accessories breaking realism.
          </h2>
        </div>
        <div class="item item-landscape">
          <img src="static/images/style.png" alt="Cross-country restyling comparison across multiple targets." loading="lazy">
          <h2 class="subtitle has-text-centered">
            Cross-country restylization leans on palette swaps and leaves Global-South identities unchanged.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!-- Key contributions -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <h2 class="title is-3 has-text-centered">Research Contributions</h2>
      <div class="columns is-multiline is-variable is-5">
        <div class="column is-half-tablet is-one-quarter-desktop is-flex">
          <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
            <h3 class="title is-5">Era-aware Benchmarking Suite</h3>
            <p class="mt-2 is-flex-grow-1">Six countries, eight categories, and 36 subcategories with traditional, modern, and era-agnostic prompts expose temporal blindspots in generative priors.</p>
          </div>
        </div>
        <div class="column is-half-tablet is-one-quarter-desktop is-flex">
          <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
            <h3 class="title is-5">Integrated T2I ✕ I2I Protocols</h3>
            <p class="mt-2 is-flex-grow-1">Base T2I generations seed three complementary editing studies—multi-loop, attribute addition, cross-country restyle—capturing how bias propagates across pipelines.</p>
          </div>
        </div>
        <div class="column is-half-tablet is-one-quarter-desktop is-flex">
          <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
            <h3 class="title is-5">Culture-aware Evaluation Signal</h3>
            <p class="mt-2 is-flex-grow-1">A retrieval-augmented VQA metric surfaces cultural drift that CLIPScore and aesthetic scores overlook, aligning tightly with native reviewer decisions.</p>
          </div>
        </div>
        <div class="column is-half-tablet is-one-quarter-desktop is-flex">
          <div class="box has-text-left is-flex is-flex-direction-column is-flex-grow-1">
            <h3 class="title is-5">Open Audit Stack</h3>
            <p class="mt-2 is-flex-grow-1">We release generations, prompts, execution configs, and the survey platform—ready to fork into future fairness checkpoints and longitudinal monitoring.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End key contributions -->

<!-- Key findings -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Observed Failure Modes</h2>
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content has-text-left">
            <ul>
              <li><strong>Global-North defaults:</strong> Country-agnostic prompts converge to U.S.-centric, modern aesthetics even with controlled schema and era cues.</li>
              <li><strong>Metric-human gap:</strong> CLIPScore and aesthetic metrics remain stable while the culture-aware signal and native raters flag rapid semantic drift.</li>
              <li><strong>Shortcut editing:</strong> Iterative edits substitute palette shifts and flags for genuine cultural attributes, often retaining the source identity for Global-South targets.</li>
              <li><strong>Demographic skew:</strong> Gender-neutral occupation prompts still surface male dominance and light skin tones, highlighting systematic data imbalance.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End key findings -->

<!-- Multi-loop progression -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-variable is-6 is-vcentered">
        <div class="column is-5">
          <h2 class="title is-3">Multi-loop Editing Drift</h2>
          <p>Five successive instructions should converge toward culturally faithful depictions. Instead, models collapse to a homogenized wedding aesthetic irrespective of the target locale.</p>
          <p class="mt-3">Traditional attire dissolves into Global-North gowns, regional symbols fade to generic décor, and palette tweaks masquerade as meaningful edits.</p>
          <p class="mt-3 is-size-6 has-text-grey">Rows correspond to countries, columns to edit iterations 0→5. Automatic metrics stay optimistic while cultural fidelity visibly deteriorates.</p>
        </div>
        <div class="column is-7">
          <div class="box has-background-white" style="overflow-x: auto;">
            <figure class="image">
              <img src="static/images/multi-loop-edit-ex.png" alt="Multi-loop edit progression across six countries showing cultural drift toward similar wedding scenes." loading="lazy">
            </figure>
            <p class="is-size-7 has-text-grey mt-2 has-text-centered">Iterative edits flatten regional diversity: prompts request country-specific weddings, but outputs converge toward the same Global-North style.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End multi-loop progression -->

<!-- Culture-aware metric -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered is-variable is-6">
        <div class="column is-half">
          <h2 class="title is-3">Culture-aware Metric Matches Native Reviewers</h2>
          <p>Our retrieval-augmented VQA metric agrees with country-native reviewers on 74% of best selections and 84% of worst selections, capturing cultural erosion that generic scores overlook.</p>
          <p class="mt-3">We ground each evaluation with Wikipedia-derived context retrieved via FAISS and question answering with Qwen2 models, enabling scalable yet culturally aware diagnostics.</p>
        </div>
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/result/best_worst_agreement_heatmap.png" alt="Heatmap showing agreement between the culture-aware metric and human best and worst selections across countries." loading="lazy">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End culture-aware metric -->

<!-- Human evaluation -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered is-variable is-6">
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/human.png" alt="Screenshots of the ECB Human Survey platform used by native expert reviewers." loading="lazy">
          </figure>
        </div>
        <div class="column is-half">
          <h2 class="title is-3">Expert-in-the-loop Evaluation Platform</h2>
          <p>Native raters evaluate image quality and cultural representation for each editing loop using the ECB Human Survey platform, ensuring judgments reflect emic expertise.</p>
          <p class="mt-3">The tool supports IRB documentation, reviewer dashboards, and best/worst selection workflows, making it easier to repeat cultural audits in new domains.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End human evaluation -->

<!-- Beyond culture -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-vcentered is-variable is-6">
        <div class="column is-half">
          <h2 class="title is-3">Beyond Culture: Occupational Bias</h2>
          <p>Even with gender-neutral prompts, occupation generations skew male for leadership roles and predominantly show light skin tones, while caregiving roles skew female.</p>
          <p class="mt-3">These findings reinforce that cultural fidelity and demographic fairness must be evaluated together when deploying generative models.</p>
        </div>
        <div class="column is-half">
          <figure class="image">
            <img src="static/images/other-bias-ex.png" alt="Examples of gender and skin-tone skew in occupation prompts produced by generative models." loading="lazy">
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End beyond culture -->





<!-- Paper poster -->
<section class="hero is-medium is-light">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <div class="columns is-variable is-6 is-vcentered">
        <div class="column is-5">
          <h2 class="title is-3">Read the Paper</h2>
          <p class="subtitle is-5">Dive into the full methodology, extended analyses, and deployment guidance.</p>
          <div class="content">
            <ul>
              <li>Full benchmark specification with prompt schema, sampling controls, and evaluation circuits.</li>
              <li>Expanded qualitative audits, survey instrumentation, and reproducible model configurations.</li>
              <li>High-resolution figures, tables, and appendices ready for downstream analysis.</li>
            </ul>
          </div>
          <div class="buttons">
            <a class="button is-link is-medium" href="static/pdfs/Exposing%20Blindspot.pdf" target="_blank">
              <span class="icon"><i class="fas fa-file-download"></i></span>
              <span>Download PDF</span>
            </a>
            <a class="button is-light is-medium" href="https://huggingface.co/datasets/seochan99/ecb-datasets" target="_blank">
              <span class="icon" aria-hidden="true">🤗</span>
              <span>View Assets</span>
            </a>
          </div>
        </div>
        <div class="column is-7">
          <div class="box has-background-white">
            <div class="has-ratio" style="position:relative;padding-top:141%;">
              <iframe src="static/pdfs/Exposing%20Blindspot.pdf" title="Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models PDF" style="border:0; position:absolute; top:0; left:0; width:100%; height:100%;"></iframe>
            </div>
            <p class="is-size-7 has-text-grey mt-2 has-text-centered">Preview the PDF in-browser or open in a new tab for a full-screen experience.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!--End paper poster -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{seo2025exposing,
  title={Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models},
  author={Seo, Huichan and Choi, Sieun and Hong, Minki and Zhou, Yi and Kim, Junseo and Ismaila, Lukman and Etori, Naome and Agarwal, Mehul and Liu, Zhixuan and Kim, Jihie and Oh, Jean},
  journal={arXiv preprint},
  year={2025},
  url={https://arxiv.org/submit/6893026/view}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
